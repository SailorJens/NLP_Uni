{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a197922e",
      "metadata": {
        "id": "a197922e"
      },
      "source": [
        "\n",
        "# Week 1 Project — From Raw Text to Features\n",
        "**Goal:** Apply tokenisation, stopword removal, lemmatization, Bag of Words, and TF‑IDF on a corpus of your choice, then compare results and reflect.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a8c9f1c",
      "metadata": {
        "id": "8a8c9f1c"
      },
      "source": [
        "## Setup (Run this)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0a08cf45",
      "metadata": {
        "id": "0a08cf45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install & imports (safe for Colab)\n",
        "!pip -q install nltk==3.9.1 scikit-learn==1.5.2 wordcloud==1.9.3\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('gutenberg', quiet=True)\n",
        "nltk.download('brown', quiet=True)\n",
        "nltk.download('reuters', quiet=True)\n",
        "nltk.download('movie_reviews', quiet=True)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import gutenberg, brown, reuters, movie_reviews, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec5ae3ba",
      "metadata": {
        "id": "ec5ae3ba"
      },
      "source": [
        "\n",
        "## 1) Choose / Load Your Corpus\n",
        "Pick **one** option below. You can also replace texts with your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6aaef6ad",
      "metadata": {
        "id": "6aaef6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2 document(s). Example preview:\n",
            " Paste or load your first document here. You can replace these strings with text from news, blogs, or your own writing.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === OPTION A: Use an NLTK corpus (UNCOMMENT exactly one) ===\n",
        "# docs = [\" \".join(brown.words(categories='news'))[:20000]]\n",
        "# docs = [\" \".join(reuters.words(fileid)) for fileid in reuters.fileids()[:10]]\n",
        "# docs = [\" \".join(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()[:10]]\n",
        "# docs = [gutenberg.raw('austen-emma.txt')[:50000]]\n",
        "\n",
        "# === OPTION B: Paste your own documents (default placeholder) ===\n",
        "docs = [\n",
        "    \"Paste or load your first document here. You can replace these strings with text from news, blogs, or your own writing.\",\n",
        "    \"Add a second document here to compare TF‑IDF across multiple docs (recommended).\"\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(docs)} document(s). Example preview:\\n\", docs[0][:300])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada468bd",
      "metadata": {
        "id": "ada468bd"
      },
      "source": [
        "\n",
        "## 2) Preprocessing — **TODO: implement your pipeline**\n",
        "Tokenise → lowercase → remove stopwords → lemmatize (and optionally compare stemming).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "20bd5a71",
      "metadata": {
        "id": "20bd5a71"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokens' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmy_preprocess\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, remove_stop: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, lemmatize: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# TODO: write your code here (about 6–10 lines)\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# TODO: tokenise\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# TODO: lemmatize\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# (Optional) try stemming:\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(tokens)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m docs_clean = \u001b[43m[\u001b[49m\u001b[43mmy_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs_clean), docs_clean[\u001b[32m0\u001b[39m][:\u001b[32m200\u001b[39m]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmy_preprocess\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, remove_stop: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, lemmatize: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# TODO: write your code here (about 6–10 lines)\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# TODO: tokenise\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# TODO: lemmatize\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# (Optional) try stemming:\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(tokens)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m docs_clean = [\u001b[43mmy_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m     22\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs_clean), docs_clean[\u001b[32m0\u001b[39m][:\u001b[32m200\u001b[39m]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mmy_preprocess\u001b[39m\u001b[34m(text, remove_stop, lemmatize)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmy_preprocess\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, remove_stop: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, lemmatize: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# TODO: write your code here (about 6–10 lines)\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# TODO: tokenise\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# TODO: lemmatize\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# (Optional) try stemming:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43mtokens\u001b[49m)\n",
            "\u001b[31mNameError\u001b[39m: name 'tokens' is not defined"
          ]
        }
      ],
      "source": [
        "# === TODO 1: Build your preprocessing function ===\n",
        "# Hints:\n",
        "# - tokens = word_tokenize(text)\n",
        "# - tokens = [t.lower() for t in tokens if t.isalpha()]\n",
        "# - remove stopwords via nltk.corpus.stopwords\n",
        "# - lemmatize with WordNetLemmatizer()\n",
        "# (Optional) Compare with PorterStemmer()\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def my_preprocess(text: str, remove_stop: bool = True, lemmatize: bool = True) -> str:\n",
        "    # TODO: write your code here (about 6–10 lines)\n",
        "    # TODO: tokenise\n",
        "    # TODO: keep alphabetic\n",
        "    # TODO: remove stopwords\n",
        "    # TODO: lemmatize\n",
        "    # (Optional) try stemming:\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "docs_clean = [my_preprocess(d) for d in docs]\n",
        "len(docs_clean), docs_clean[0][:200]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd41300a",
      "metadata": {
        "id": "cd41300a"
      },
      "source": [
        "###  Quick Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e37cb308",
      "metadata": {
        "id": "e37cb308"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    wc = WordCloud(width=900, height=400, background_color=\"white\").generate(\" \".join(docs_clean))\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.imshow(wc); plt.axis(\"off\")\n",
        "    plt.title(\"Word Cloud (preprocessed)\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"WordCloud skipped:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee843de",
      "metadata": {
        "id": "dee843de"
      },
      "source": [
        "## 3) Bag of Words — **TODO** build and inspect top words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a33186",
      "metadata": {
        "id": "53a33186"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === TODO 2: Create a Bag of Words vectorizer and fit ===\n",
        "# Hint: CountVectorizer(max_features=2000, ngram_range=(1,1))\n",
        "# bow_vec = ...\n",
        "# X_bow = ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f32f0d6",
      "metadata": {
        "id": "9f32f0d6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot top BoW terms\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(top_terms, top_counts)\n",
        "plt.title(\"Bag of Words — Top Terms\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0287b1b8",
      "metadata": {
        "id": "0287b1b8"
      },
      "source": [
        "## 4) TF‑IDF — **TODO** build and inspect top weighted words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093d64dd",
      "metadata": {
        "id": "093d64dd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === TODO 3: Create a TF-IDF vectorizer and fit ===\n",
        "# Hint: TfidfVectorizer(max_features=2000, ngram_range=(1,1))\n",
        "# tfidf_vec = ...\n",
        "# X_tfidf = ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c53706",
      "metadata": {
        "id": "e0c53706"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot top TF‑IDF terms\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(top_terms_t, top_scores_t)\n",
        "plt.title(\"TF‑IDF — Top Weighted Terms\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"TF‑IDF score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a28a8d",
      "metadata": {
        "id": "b4a28a8d"
      },
      "source": [
        "\n",
        "## Submission Checklist\n",
        "- [ ] Notebook runs end-to-end  \n",
        "- [ ] Preprocessing function implemented by you  \n",
        "- [ ] BoW top terms + plot  \n",
        "- [ ] TF‑IDF top terms + plot  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
