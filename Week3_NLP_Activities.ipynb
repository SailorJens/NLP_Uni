{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "166faf3d",
      "metadata": {
        "id": "166faf3d"
      },
      "source": [
        "\n",
        "# Week 3: NLP Foundations: Text Generation\n",
        "\n",
        "This lab runs smoothly on **Google Colab**. It contains **two coding-only exercises**:\n",
        "- **Exercise 1 (heavily scaffolded)**: Use a **Transformer** (GPT-style causal language model) for **text completion**.\n",
        "- **Exercise 2 (lightly scaffolded)**: Use the same model to **generate variations at different `temperature` values**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7ca0e2",
      "metadata": {
        "id": "9b7ca0e2"
      },
      "source": [
        "\n",
        "## Before You Start: Use a GPU on Google Colab\n",
        "\n",
        "1. Go to **Runtime ▶ Change runtime type**.\n",
        "2. Set **Hardware accelerator** to **T4 GPU**, then click **Save**.\n",
        "3. Run the cell below to confirm Colab sees your GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a72067e",
      "metadata": {
        "id": "5a72067e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Check GPU availability in Colab\n",
        "import torch, os, sys, subprocess\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
        "    # Optional: show GPU details\n",
        "    try:\n",
        "        subprocess.run([\"nvidia-smi\"], check=False)\n",
        "    except Exception as e:\n",
        "        print(\"nvidia-smi not available:\", e)\n",
        "else:\n",
        "    print(\"Tip: In Colab, go to Runtime > Change runtime type > GPU, then rerun this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d1b015d",
      "metadata": {
        "id": "2d1b015d"
      },
      "source": [
        "**bold text**\n",
        "## Setup\n",
        "\n",
        "This will install and import the required libraries. Rerun if the runtime restarts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c24a69",
      "metadata": {
        "id": "19c24a69"
      },
      "outputs": [],
      "source": [
        "\n",
        "# If running on Colab, install dependencies quickly.\n",
        "def in_colab():\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "if in_colab():\n",
        "    # Colab usually has a recent torch. Upgrade transformers.\n",
        "    !pip -q install transformers==4.44.2 accelerate\n",
        "else:\n",
        "    print(\"Running outside Colab. Ensure you have: transformers>=4.44.2, accelerate, torch.\")\n",
        "\n",
        "import torch, random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54123cba",
      "metadata": {
        "id": "54123cba"
      },
      "source": [
        "\n",
        "## What Are We Using? Quick Concepts\n",
        "\n",
        "- **Transformer (GPT-style, causal language model)**: A neural network that predicts the next token given previous tokens. \"Causal\" means it only attends to the left (past context), which is ideal for **text generation and completion**.\n",
        "- **Tokenizer**: Converts text to token IDs and back. Models operate on token IDs.\n",
        "- **Generation**: We provide a **prompt** and ask the model to produce **`max_new_tokens`** tokens.\n",
        "\n",
        "### Key Generation Parameters\n",
        "- **`max_new_tokens`**: How many tokens to generate beyond your prompt.\n",
        "- **`temperature`**: Controls randomness during sampling.\n",
        "  - `0.0` ~ deterministic (with greedy decoding); higher values produce more diverse outputs.\n",
        "- **`do_sample`**: Whether to sample from a distribution instead of picking the single most likely token.\n",
        "  - `False` = **greedy decoding** (deterministic, may be repetitive or dull).\n",
        "  - `True` = **sampling** (enables `temperature`, `top_k`, `top_p`).\n",
        "- **`top_k`**: Sample only from the **top K** highest-probability tokens at each step (e.g., 50).\n",
        "- **`top_p` (nucleus sampling)**: Sample from the **smallest set of tokens whose cumulative probability ≥ p** (e.g., 0.9 or 0.95).\n",
        "- **`no_repeat_ngram_size`**: Discourages repeating n-grams of a given size (e.g., 3).\n",
        "- **`eos_token_id`**: Token ID that signals \"end of sequence\". If the model produces it, generation stops early.\n",
        "\n",
        "> For this lab we will use **`distilgpt2`**, a small GPT-2 style model that runs on CPU or GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79ffcf98",
      "metadata": {
        "id": "79ffcf98"
      },
      "source": [
        "\n",
        "## Exercise 1 — Heavily Scaffolded: Text Completion with a Transformer\n",
        "\n",
        "**Goal**: Load a GPT-style model and complete a prompt. Then tweak parameters to observe differences.\n",
        "\n",
        "**What to do**: Just **run the cells**. Then **change the parameter values** and re-run to see effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa7ec01",
      "metadata": {
        "id": "efa7ec01"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1) Load a small, CPU/GPU-friendly model\n",
        "MODEL_NAME = \"distilgpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "text_gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if device == \"cuda\" else -1,\n",
        ")\n",
        "\n",
        "print(\"Model loaded:\", MODEL_NAME, \"| Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f070e39",
      "metadata": {
        "id": "9f070e39"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 2) Provide a prompt to complete\n",
        "prompt = \"In natural language processing, a tokenizer is\"\n",
        "\n",
        "# 3) Generation parameters (feel free to change and re-run)\n",
        "params = {\n",
        "    \"max_new_tokens\": 50,\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "outputs = text_gen(prompt, **params)\n",
        "print(outputs[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a4848e",
      "metadata": {
        "id": "f7a4848e"
      },
      "source": [
        "\n",
        "### Try These Quick Variations (Run and Compare)\n",
        "- Set **`do_sample=False`** (greedy decoding). Remove `top_k` and `top_p` from `params`. How does the output change?\n",
        "- Increase **`max_new_tokens`** to see when the model starts to ramble.\n",
        "- Set **`temperature=0.0`** with `do_sample=True` (still near-deterministic sampling) vs **`temperature=1.3`** for more surprising outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f8731d",
      "metadata": {
        "id": "f8f8731d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Optional: Batch generation for multiple prompts\n",
        "prompts = [\n",
        "    \"Transformers revolutionised NLP because\",\n",
        "    \"Attention mechanisms allow models to\",\n",
        "    \"Fine-tuning a language model involves\",\n",
        "]\n",
        "\n",
        "batch_outputs = text_gen(prompts, **params)\n",
        "for i, out in enumerate(batch_outputs):\n",
        "    print(f\"--- Prompt {i+1} ---\")\n",
        "    print(out[0][\"generated_text\"], end=\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0671fdab",
      "metadata": {
        "id": "0671fdab"
      },
      "source": [
        "\n",
        "## Exercise 2 — Lightly Scaffolded: Explore `temperature`\n",
        "\n",
        "**Goal**: Generate multiple continuations from the **same prompt** at different temperatures and observe the differences.\n",
        "\n",
        "**What to do**: Just **run the cell**. Then **change the temperatures** and re-run to see how style and variability change.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "975680a5",
      "metadata": {
        "id": "975680a5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import List\n",
        "\n",
        "def generate_with_temperature(prompt: str, temps: List[float], max_new_tokens: int = 60):\n",
        "    results = {}\n",
        "    for t in temps:\n",
        "        outputs = text_gen(\n",
        "            prompt,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=t,\n",
        "            do_sample=(t > 0.0),  # if t==0.0 we effectively get greedy behavior\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            no_repeat_ngram_size=3,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        results[t] = outputs[0][\"generated_text\"]\n",
        "    return results\n",
        "\n",
        "# Your experiment (feel free to change)\n",
        "prompt = \"Write a short, vivid description of a sunrise over a quiet city.\"\n",
        "temperatures = [0.0, 0.3, 0.7, 1.0, 1.3]\n",
        "\n",
        "results = generate_with_temperature(prompt, temperatures)\n",
        "\n",
        "for t, text in results.items():\n",
        "    print(f\"===== Temperature = {t} =====\")\n",
        "    print(text, end=\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8fa6826",
      "metadata": {
        "id": "a8fa6826"
      },
      "source": [
        "\n",
        "## (Optional, Instructor Demo) Using a Hosted API Model\n",
        "\n",
        "If you have API access, you can compare with a larger chat model. This is **optional** and **commented out**. Do **not** share your API key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed45b2c",
      "metadata": {
        "id": "0ed45b2c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# OPTIONAL: Requires `openai` >= 1.0.0\n",
        "# %pip -q install openai\n",
        "\n",
        "# from openai import OpenAI\n",
        "# import os\n",
        "# os.environ['OPENAI_API_KEY'] = \"YOUR_KEY_HERE\"  # or set securely in the environment\n",
        "# client = OpenAI()\n",
        "\n",
        "# def chat_generate(prompt: str, temperature: float = 0.7, model: str = \"gpt-4o-mini\"):\n",
        "#     resp = client.chat.completions.create(\n",
        "#         model=model,\n",
        "#         temperature=temperature,\n",
        "#         messages=[\n",
        "#             {\"role\": \"system\", \"content\": \"You are a helpful writing assistant.\"},\n",
        "#             {\"role\": \"user\", \"content\": prompt},\n",
        "#         ],\n",
        "#         max_tokens=150,\n",
        "#     )\n",
        "#     return resp.choices[0].message.content\n",
        "\n",
        "# for t in [0.0, 0.3, 0.7, 1.0, 1.3]:\n",
        "#     print(f\"=== Temperature {t} ===\")\n",
        "#     print(chat_generate(\"Write a 3-sentence micro-story about a lost key.\", temperature=t))\n",
        "#     print()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}