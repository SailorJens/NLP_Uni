{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "26e243d3",
      "metadata": {
        "id": "26e243d3"
      },
      "source": [
        "\n",
        "# Week 3 Project: Lyric Style Transfer\n",
        " You must **complete the TODOs** and submit this single notebook.\n",
        "\n",
        "**Goal:** Build a small pipeline that rewrites a **short** excerpt of modern lyrics (e.g., *Queen*, *Taylor Swift*, or **any artist you prefer**) in **Shakespearean** style using a **local open-source Transformer** (no API keys).\n",
        "\n",
        "> **Copyright note:** Lyrics are copyrighted. Use only **short excerpts** (a few lines) or your own paraphrases for classroom use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd91668b",
      "metadata": {
        "id": "fd91668b"
      },
      "source": [
        "\n",
        "## Before You Start â€” Enable GPU in Colab\n",
        "1. Go to **Runtime â–¶ Change runtime type**.  \n",
        "2. Set **Hardware accelerator** to **GPU**, click **Save**.  \n",
        "3. Run the GPU check below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a12e342a",
      "metadata": {
        "id": "a12e342a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# GPU check\n",
        "import torch, subprocess, sys\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        subprocess.run([\"nvidia-smi\"], check=False)\n",
        "    except Exception as e:\n",
        "        print(\"nvidia-smi not available:\", e)\n",
        "else:\n",
        "    print(\"Tip: In Colab, go to Runtime > Change runtime type > GPU, then rerun.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78563090",
      "metadata": {
        "id": "78563090"
      },
      "source": [
        "\n",
        "##  Setup â€” Install & Imports\n",
        "If running on Colab, install dependencies. Then import modules and set the device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b47c56e",
      "metadata": {
        "id": "7b47c56e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def in_colab():\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "if in_colab():\n",
        "    %pip -q install transformers==4.44.2 accelerate==0.34.2\n",
        "else:\n",
        "    print(\"Running outside Colab: ensure transformers>=4.44.2, accelerate installed.\")\n",
        "\n",
        "import random, math, re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "random.seed(7); torch.manual_seed(7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8b1f3f",
      "metadata": {
        "id": "5f8b1f3f"
      },
      "source": [
        "\n",
        "## 1) Model Choice (Local â€” No API)\n",
        "Pick one **open model** to run locally:\n",
        "- `\"microsoft/phi-2\"` â€” strong quality (needs GPU, T4 OK)\n",
        "- `\"gpt2-medium\"` â€” decent & lighter\n",
        "- `\"distilgpt2\"` â€” smallest; use if limited compute\n",
        "\n",
        "> If you get out-of-memory errors, switch to a smaller model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9bd4466",
      "metadata": {
        "id": "d9bd4466"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === TODO 3: Choose model and load ===\n",
        "# Steps:\n",
        "#   1) Set MODEL_NAME = \"...\"\n",
        "#   2) Load tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "#   3) Load model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
        "#   4) Build a text-generation pipeline named 'text_gen' with the model & tokenizer.\n",
        "#      device index: 0 if cuda else -1\n",
        "#   5) Print a confirmation (model name + device).\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a62879f3",
      "metadata": {
        "id": "a62879f3"
      },
      "source": [
        "\n",
        "## 2) Prompt Design â€” Shakespearean Style\n",
        "Create a function `build_prompt(text: str) -> str` that instructs the model to rewrite the input in Shakespearean style (Early Modern English: thee, thou, thy), avoid modern slang, keep the meaning, and **output only the rewritten text**, make sure to include the orinigal lyrics int the promt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5478ee",
      "metadata": {
        "id": "2d5478ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === TODO 4: Prompt template function ===\n",
        "# Define build_prompt(text: str) -> str\n",
        "# Print a preview for a tiny sample.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#def build_prompt(text: str):\n",
        "#    return (\n",
        "#        \"WRITE THE PROMP\"\n",
        "#    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424a676a",
      "metadata": {
        "id": "424a676a"
      },
      "source": [
        "\n",
        "## 3) Input â€” Short Lyric Excerpt\n",
        "Paste a **short excerpt** (a few lines) or a paraphrase here.  \n",
        "You may use **Queen** or **any artist you like**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af86716",
      "metadata": {
        "id": "3af86716"
      },
      "outputs": [],
      "source": [
        "\n",
        "lyric_excerpt = \"\"\"\n",
        "Is this the real life? Is this just fantasy?\n",
        "Caught in a landslide, no escape from reality\n",
        "Open your eyes, look up to the skies and see\n",
        "I'm just a poor boy, I need no sympathy\n",
        "\"\"\"\n",
        "\n",
        "# Or complete with lyrics of your choice\n",
        "# lyrics_excerpt = \" \"\n",
        "\n",
        "print(\"Excerpt length (chars):\", len(lyric_excerpt))\n",
        "print(lyric_excerpt[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2d184b7",
      "metadata": {
        "id": "f2d184b7"
      },
      "source": [
        "\n",
        "## 4) Generation â€” First Rewrite\n",
        "Implement a function `generate_text(prompt: str, **kwargs)` that calls your pipeline with parameters like:\n",
        "- `max_new_tokens` (e.g., 120â€“200)\n",
        "- `do_sample=True`\n",
        "- `temperature` (e.g., 0.7â€“1.0)\n",
        "- `top_k` (e.g., 50)\n",
        "- `top_p` (e.g., 0.9â€“0.95)\n",
        "- `no_repeat_ngram_size` (e.g., 3)\n",
        "- `eos_token_id` (use tokenizer.eos_token_id)\n",
        "\n",
        "Then generate one rewrite and print it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c579aac",
      "metadata": {
        "id": "5c579aac"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === TODO 6: Generation function & single run ===\n",
        "# 1) Define generate_text(prompt: str, **kwargs) -> str (use 'text_gen(...)[0][\"generated_text\"]')\n",
        "# 2) Create default params in a dict 'gen_defaults' (include temperature, top_k, top_p, etc.)\n",
        "# 3) Build prompt from your excerpt and call generate_text once. Print the result.\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b658d23",
      "metadata": {
        "id": "5b658d23"
      },
      "source": [
        "\n",
        "## 5) Experiment â€” Temperature Sweep\n",
        "Generate multiple variants at different temperatures to observe diversity vs. coherence.\n",
        "\n",
        "**Requirement:** produce outputs for at least **3 temperatures** (e.g., 0.0, 0.7, 1.2).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c25e8484",
      "metadata": {
        "id": "c25e8484"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === TODO 7: Temperature sweep ===\n",
        "# 1) Create a list of temperatures (e.g., [0.0, 0.7, 1.2])\n",
        "# 2) Loop over them, update temperature in params, and print outputs with clear headers.\n",
        "#    Example print header: f\"===== Temperature = {t} =====\"\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f76b489",
      "metadata": {
        "id": "5f76b489"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## ðŸ“¦ What to Submit\n",
        "- This **single `.ipynb` notebook**, completed and runnable **top-to-bottom**.\n",
        "- Generated outputs for **â‰¥ 3 temperatures** printed in the notebook.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}