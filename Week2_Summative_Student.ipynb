{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "690caef9",
      "metadata": {
        "id": "690caef9"
      },
      "source": [
        "\n",
        "# Summative Exercise – Predictive Embeddings + RNN Classifier (Colab)\n",
        "\n",
        "**Week 2 (NLP):** This summative task combines *predictive embeddings* (loaded from pre-trained **GloVe**, a Word2Vec-style model) with a compact **RNN** text classifier.\n",
        "\n",
        "**You will:**\n",
        "1. Load a small two-class dataset (20 Newsgroups subset).  \n",
        "2. Tokenise and create padded sequences.  \n",
        "3. Load pre-trained GloVe vectors and explore nearest neighbours.  \n",
        "4. Build two models: (i) averaged-embedding baseline; (ii) SimpleRNN classifier using the same embeddings.  \n",
        "5. Evaluate, compare, and reflect.\n",
        "\n",
        "Note: You have complete all the missing parts in code to complete this exercise.\n",
        "Missing parts are represented with '#########'.\n",
        "\n",
        "*Estimated time: 90 minutes.*  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "646c455a",
      "metadata": {
        "id": "646c455a"
      },
      "source": [
        "## Setup and versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941326dc",
      "metadata": {
        "id": "941326dc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, random, sys, platform, numpy as np, tensorflow as tf, sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"scikit-learn:\", sklearn.__version__)\n",
        "print(\"Platform:\", platform.platform())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a880962c",
      "metadata": {
        "id": "a880962c"
      },
      "source": [
        "\n",
        "## 1. Load and prepare data\n",
        "\n",
        "We use two categories to keep training fast. This mirrors the earlier classroom exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f08a2f3",
      "metadata": {
        "id": "6f08a2f3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "cats = ['rec.autos', 'sci.electronics']\n",
        "raw = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "df = pd.DataFrame({'text': raw.data, 'label': raw.target}).sample(n=1000, random_state=SEED).reset_index(drop=True)\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1fbf50",
      "metadata": {
        "id": "6d1fbf50"
      },
      "source": [
        "\n",
        "## 2. Tokenise and vectorise\n",
        "\n",
        "We use Keras `Tokenizer` for simple, robust tokenisation. Then we create padded integer sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28273aa2",
      "metadata": {
        "id": "28273aa2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Hint: try using 20000 words and maximum length of 120\n",
        "\n",
        "num_words = #############\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(##################)\n",
        "\n",
        "max_len = ##################\n",
        "seqs = tokenizer.texts_to_sequences(df[\"text\"])\n",
        "X = pad_sequences(seqs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "y = #########################  # labels\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "index_word = {i:w for w,i in word_index.items()}\n",
        "vocab_size = min(num_words, len(word_index) + 1)\n",
        "vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8076897",
      "metadata": {
        "id": "e8076897"
      },
      "outputs": [],
      "source": [
        "# Hint: 20% of data can go for testing\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=############, random_state=SEED, stratify=y\n",
        ")\n",
        "X_train.shape, X_val.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "994a87a0",
      "metadata": {
        "id": "994a87a0"
      },
      "source": [
        "\n",
        "## 3. Load pre-trained predictive embeddings (GloVe)\n",
        "\n",
        "We use **GloVe 6B** via Keras. If the download fails, you can continue with random initialisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4103d89",
      "metadata": {
        "id": "c4103d89"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, zipfile\n",
        "from tensorflow.keras.utils import get_file\n",
        "import numpy as np\n",
        "\n",
        "EMBED_DIM = 100\n",
        "GLOVE_URL = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "glove_zip_path = get_file(\"glove.6B.zip\", GLOVE_URL, cache_dir=\".\", cache_subdir=\".\")\n",
        "if not os.path.exists(\"glove.6B.100d.txt\"):\n",
        "    with zipfile.ZipFile(glove_zip_path, \"r\") as z:\n",
        "        z.extract(\"glove.6B.100d.txt\", path=\".\")\n",
        "\n",
        "emb_index = {}\n",
        "with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        parts = ###########\n",
        "        word = ############\n",
        "        vec = ############\n",
        "        emb_index[word] = vec\n",
        "\n",
        "embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, EMBED_DIM)).astype(\"float32\")\n",
        "hits = 0\n",
        "for word, idx in word_index.items():\n",
        "    if idx >= vocab_size:\n",
        "        continue\n",
        "    vec = emb_index.get(word)\n",
        "    if vec is not None and vec.shape[0] == EMBED_DIM:\n",
        "        embedding_matrix[idx] = vec\n",
        "        hits += 1\n",
        "print(f\"Loaded embeddings: {len(emb_index)} | Vocab hits: {hits}/{vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d6bf1a",
      "metadata": {
        "id": "e3d6bf1a"
      },
      "source": [
        "\n",
        "## 4. Explore nearest neighbours (cosine similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac3461f0",
      "metadata": {
        "id": "ac3461f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def nearest_neighbours(query, topk=10):\n",
        "    ########\n",
        "    ########\n",
        "    ########\n",
        "    ########\n",
        "    ################\n",
        "    ########\n",
        "    ########\n",
        "    ########\n",
        "\n",
        "probes = [\"car\", \"engine\", \"battery\", \"circuit\", \"voltage\"]\n",
        "for p in probes:\n",
        "    print(f\"Probe: {p}\")\n",
        "    print(nearest_neighbours(p, topk=8))\n",
        "    print(\"-\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb91517f",
      "metadata": {
        "id": "eb91517f"
      },
      "source": [
        "\n",
        "## 5. Baseline: averaged embeddings + Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f377f0d7",
      "metadata": {
        "id": "f377f0d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Hint: Iteration could be 300.\n",
        "\n",
        "def doc_mean_vector(seq_row):\n",
        "    valid = [embedding_matrix[idx] for idx in seq_row if idx != 0 and idx < vocab_size]\n",
        "    if not valid:\n",
        "        return np.zeros((EMBED_DIM,), dtype=\"float32\")\n",
        "    return np.mean(valid, axis=0)\n",
        "\n",
        "X_tr_mean = np.stack([doc_mean_vector(r) for r in X_train])\n",
        "X_va_mean = np.stack([doc_mean_vector(r) for r in ###############])\n",
        "\n",
        "clf = LogisticRegression(max_iter=#############)\n",
        "clf.fit(X_tr_mean, y_train)\n",
        "pred_lr = clf.predict(X_va_mean)\n",
        "\n",
        "acc_lr = accuracy_score(###########)\n",
        "f1_lr = f1_score(##########, #######, average=\"macro\")\n",
        "print(f\"Baseline (avg embeddings) | acc={acc_lr:.3f} | f1_macro={f1_lr:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6b12a5",
      "metadata": {
        "id": "cd6b12a5"
      },
      "source": [
        "\n",
        "## 6. RNN classifier with pre-trained embeddings (Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31fc70e8",
      "metadata": {
        "id": "31fc70e8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# Hint: For dense layer use sigmoid as activation function. Use adam optimizer and loss should be binary crossentropy.\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=EMBED_DIM,\n",
        "              input_length=X.shape[1], weights=[embedding_matrix],\n",
        "              trainable=False),\n",
        "    SimpleRNN(64, activation=\"tanh\"),\n",
        "    Dense(1, activation=############)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=########, loss=###########, metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9248fc",
      "metadata": {
        "id": "ad9248fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10, batch_size=32, verbose=1\n",
        ")\n",
        "\n",
        "y_pred_prob = model.predict(##########, verbose=0).ravel()\n",
        "y_pred = (y_pred_prob >= 0.5).astype(\"int64\")\n",
        "\n",
        "acc_rnn = accuracy_score(y_val, ############)\n",
        "f1_rnn = f1_score(y_val, y_pred, average=\"macro\")\n",
        "print(f\"RNN (frozen GloVe) | acc={acc_rnn:.3f} | f1_macro={f1_rnn:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "760d942b",
      "metadata": {
        "id": "760d942b"
      },
      "source": [
        "## 7. Compare results & confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d41b2f3",
      "metadata": {
        "id": "2d41b2f3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "results = pd.DataFrame([\n",
        "    {\"model\": \"Avg-embeddings + LogisticRegression\", \"accuracy\": acc_lr, \"f1_macro\": f1_lr},\n",
        "    {\"model\": \"SimpleRNN (frozen GloVe)\", \"accuracy\": acc_rnn, \"f1_macro\": f1_rnn},\n",
        "])\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b37d35",
      "metadata": {
        "id": "29b37d35"
      },
      "outputs": [],
      "source": [
        "\n",
        "cm = confusion_matrix(################)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['rec.autos','sci.electronics'])\n",
        "disp.plot(values_format='d')\n",
        "plt.title(\"Confusion matrix: RNN (frozen GloVe)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "372c8eb3",
      "metadata": {
        "id": "372c8eb3"
      },
      "source": [
        "\n",
        "## 8. Reflection (120–180 words)\n",
        "\n",
        "- Where did the averaged-embedding baseline do well, and where did the RNN improve?\n",
        "- Why do predictive embeddings help compared with raw counts or TF-IDF?  \n",
        "- If you fine-tuned the embedding layer (`trainable=True`) in this small-data setting, when would you expect gains or overfitting, and why?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FcRcJPR6HeaE",
      "metadata": {
        "id": "FcRcJPR6HeaE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.11.13)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
