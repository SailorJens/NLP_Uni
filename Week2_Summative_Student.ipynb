{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "690caef9",
      "metadata": {
        "id": "690caef9"
      },
      "source": [
        "\n",
        "# Summative Exercise – Predictive Embeddings + RNN Classifier (Colab)\n",
        "\n",
        "**Week 2 (NLP):** This summative task combines *predictive embeddings* (loaded from pre-trained **GloVe**, a Word2Vec-style model) with a compact **RNN** text classifier.\n",
        "\n",
        "**You will:**\n",
        "1. Load a small two-class dataset (20 Newsgroups subset).  \n",
        "2. Tokenise and create padded sequences.  \n",
        "3. Load pre-trained GloVe vectors and explore nearest neighbours.  \n",
        "4. Build two models: (i) averaged-embedding baseline; (ii) SimpleRNN classifier using the same embeddings.  \n",
        "5. Evaluate, compare, and reflect.\n",
        "\n",
        "Note: You have complete all the missing parts in code to complete this exercise.\n",
        "Missing parts are represented with '#########'.\n",
        "\n",
        "*Estimated time: 90 minutes.*  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "646c455a",
      "metadata": {
        "id": "646c455a"
      },
      "source": [
        "## Setup and versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "941326dc",
      "metadata": {
        "id": "941326dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.10.19\n",
            "TensorFlow: 2.20.0\n",
            "NumPy: 1.26.4\n",
            "scikit-learn: 1.5.1\n",
            "Platform: macOS-15.6.1-arm64-arm-64bit\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, random, sys, platform, numpy as np, tensorflow as tf, sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"scikit-learn:\", sklearn.__version__)\n",
        "print(\"Platform:\", platform.platform())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a880962c",
      "metadata": {
        "id": "a880962c"
      },
      "source": [
        "\n",
        "## 1. Load and prepare data\n",
        "\n",
        "We use two categories to keep training fast. This mirrors the earlier classroom exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6f08a2f3",
      "metadata": {
        "id": "6f08a2f3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># 74S\\tLater modification of 74 for even highe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\nrecently-manufactured locomotives have wheel...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\nYes, Fred, my heart and prayers go out to th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  # 74S\\tLater modification of 74 for even highe...      1\n",
              "1  \\nrecently-manufactured locomotives have wheel...      0\n",
              "2  \\nYes, Fred, my heart and prayers go out to th...      0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "cats = ['rec.autos', 'sci.electronics']\n",
        "raw = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "df = pd.DataFrame({'text': raw.data, 'label': raw.target}).sample(n=1000, random_state=SEED).reset_index(drop=True)\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1fbf50",
      "metadata": {
        "id": "6d1fbf50"
      },
      "source": [
        "\n",
        "## 2. Tokenise and vectorise\n",
        "\n",
        "We use Keras `Tokenizer` for simple, robust tokenisation. Then we create padded integer sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28273aa2",
      "metadata": {
        "id": "28273aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2750  521 1805 ... 4246 6215    7]\n",
            " [ 539 1808 4250 ...    0    0    0]\n",
            " [ 302 2753   27 ...  198   26   43]\n",
            " ...\n",
            " [   5   28  160 ...    0    0    0]\n",
            " [  49  769  337 ...    0    0    0]\n",
            " [   0    0    0 ...    0    0    0]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "12193"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Hint: try using 20000 words and maximum length of 120\n",
        "\n",
        "num_words = 20000\n",
        "# Least frequent words (not in 20k vocab) will be bucketed as <unk>\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(df[\"text\"])\n",
        "\n",
        "max_len = 120\n",
        "# Convert texts to integer sequences (indices of the vocab) and pad them to the same length of 120 (adds 0s or cuts off, at 120, resp.), save that in X, label in y\n",
        "# rare words get the <unk> index\n",
        "seqs = tokenizer.texts_to_sequences(df[\"text\"])\n",
        "X = pad_sequences(seqs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "y = df[\"label\"].values  # labels\n",
        "\n",
        "# word_index is the text vocabulary for ALL words (incl. rare words!) + <unk>\n",
        "word_index = tokenizer.word_index\n",
        "index_word = {i:w for w,i in word_index.items()}\n",
        "vocab_size = min(num_words, len(word_index) + 1)\n",
        "vocab_size\n",
        "# vocab_size < num_words, so no <unk> needed!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e8076897",
      "metadata": {
        "id": "e8076897"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((800, 120), (200, 120))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hint: 20% of data can go for testing\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "X_train.shape, X_val.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "994a87a0",
      "metadata": {
        "id": "994a87a0"
      },
      "source": [
        "\n",
        "## 3. Load pre-trained predictive embeddings (GloVe)\n",
        "\n",
        "We use **GloVe 6B** via Keras. If the download fails, you can continue with random initialisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4103d89",
      "metadata": {
        "id": "c4103d89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded embeddings: 400000 | Vocab hits: 10224/12193\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, zipfile\n",
        "from tensorflow.keras.utils import get_file\n",
        "import numpy as np\n",
        "\n",
        "EMBED_DIM = 100\n",
        "# GLOVE_URL = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "# glove_zip_path = get_file(\"glove.6B.zip\", GLOVE_URL, cache_dir=\".\", cache_subdir=\".\")\n",
        "# if not os.path.exists(\"glove.6B.100d.txt\"):\n",
        "#     with zipfile.ZipFile(glove_zip_path, \"r\") as z:\n",
        "#         z.extract(\"glove.6B.100d.txt\", path=\".\")\n",
        "\n",
        "# load GloVe embeddings into a dictionary\n",
        "emb_index = {}\n",
        "with open(\"data/glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        # parses the vector file line by line, the first part (part[0]) is the word, the rest (part[1:]) are the vector values\n",
        "        parts = line.strip().split()\n",
        "        word = parts[0]\n",
        "        vec = np.array(parts[1:], dtype=\"float32\")\n",
        "        # store in dictionary word -> vector\n",
        "        emb_index[word] = vec\n",
        "\n",
        "\n",
        "embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, EMBED_DIM)).astype(\"float32\")\n",
        "hits = 0\n",
        "for word, idx in word_index.items():\n",
        "    if idx >= vocab_size:\n",
        "        continue\n",
        "    vec = emb_index.get(word)\n",
        "    if vec is not None and vec.shape[0] == EMBED_DIM:\n",
        "        embedding_matrix[idx] = vec\n",
        "        hits += 1\n",
        "print(f\"Loaded embeddings: {len(emb_index)} | Vocab hits: {hits}/{vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d6bf1a",
      "metadata": {
        "id": "e3d6bf1a"
      },
      "source": [
        "\n",
        "## 4. Explore nearest neighbours (cosine similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ac3461f0",
      "metadata": {
        "id": "ac3461f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probe: car\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m probes:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mnearest_neighbours\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
            "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mnearest_neighbours\u001b[0;34m(query, topk)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# qv = query vector\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m qv \u001b[38;5;241m=\u001b[39m \u001b[43membedding_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m sims \u001b[38;5;241m=\u001b[39m embedding_matrix \u001b[38;5;241m@\u001b[39m qv \u001b[38;5;241m/\u001b[39m (norm(embedding_matrix, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m norm(qv) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m     10\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39msims)[:topk\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ],
      "source": [
        "\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def nearest_neighbours(query, topk=10):\n",
        "    if query not in emb_index:\n",
        "        return []\n",
        "    # qv = query vector\n",
        "    qv = embedding_matrix[query]\n",
        "    \n",
        "    sims = embedding_matrix @ qv / (norm(embedding_matrix, axis=1) * norm(qv) + 1e-8)\n",
        "    idx = np.argsort(-sims)[:topk+1]\n",
        "    return [(index_word[i], float(sims[i])) for i in idx if index_word[i] != query][:topk]\n",
        "    ########\n",
        "    ########\n",
        "    ########\n",
        "    ########\n",
        "    ################\n",
        "    ########\n",
        "    ########\n",
        "    ########\n",
        "\n",
        "probes = [\"car\", \"engine\", \"battery\", \"circuit\", \"voltage\"]\n",
        "for p in probes:\n",
        "    print(f\"Probe: {p}\")\n",
        "    print(nearest_neighbours(p, topk=8))\n",
        "    print(\"-\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb91517f",
      "metadata": {
        "id": "eb91517f"
      },
      "source": [
        "\n",
        "## 5. Baseline: averaged embeddings + Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f377f0d7",
      "metadata": {
        "id": "f377f0d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Hint: Iteration could be 300.\n",
        "\n",
        "def doc_mean_vector(seq_row):\n",
        "    valid = [embedding_matrix[idx] for idx in seq_row if idx != 0 and idx < vocab_size]\n",
        "    if not valid:\n",
        "        return np.zeros((EMBED_DIM,), dtype=\"float32\")\n",
        "    return np.mean(valid, axis=0)\n",
        "\n",
        "X_tr_mean = np.stack([doc_mean_vector(r) for r in X_train])\n",
        "X_va_mean = np.stack([doc_mean_vector(r) for r in ###############])\n",
        "\n",
        "clf = LogisticRegression(max_iter=#############)\n",
        "clf.fit(X_tr_mean, y_train)\n",
        "pred_lr = clf.predict(X_va_mean)\n",
        "\n",
        "acc_lr = accuracy_score(###########)\n",
        "f1_lr = f1_score(##########, #######, average=\"macro\")\n",
        "print(f\"Baseline (avg embeddings) | acc={acc_lr:.3f} | f1_macro={f1_lr:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6b12a5",
      "metadata": {
        "id": "cd6b12a5"
      },
      "source": [
        "\n",
        "## 6. RNN classifier with pre-trained embeddings (Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31fc70e8",
      "metadata": {
        "id": "31fc70e8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# Hint: For dense layer use sigmoid as activation function. Use adam optimizer and loss should be binary crossentropy.\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=EMBED_DIM,\n",
        "              input_length=X.shape[1], weights=[embedding_matrix],\n",
        "              trainable=False),\n",
        "    SimpleRNN(64, activation=\"tanh\"),\n",
        "    Dense(1, activation=############)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=########, loss=###########, metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9248fc",
      "metadata": {
        "id": "ad9248fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10, batch_size=32, verbose=1\n",
        ")\n",
        "\n",
        "y_pred_prob = model.predict(##########, verbose=0).ravel()\n",
        "y_pred = (y_pred_prob >= 0.5).astype(\"int64\")\n",
        "\n",
        "acc_rnn = accuracy_score(y_val, ############)\n",
        "f1_rnn = f1_score(y_val, y_pred, average=\"macro\")\n",
        "print(f\"RNN (frozen GloVe) | acc={acc_rnn:.3f} | f1_macro={f1_rnn:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "760d942b",
      "metadata": {
        "id": "760d942b"
      },
      "source": [
        "## 7. Compare results & confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d41b2f3",
      "metadata": {
        "id": "2d41b2f3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "results = pd.DataFrame([\n",
        "    {\"model\": \"Avg-embeddings + LogisticRegression\", \"accuracy\": acc_lr, \"f1_macro\": f1_lr},\n",
        "    {\"model\": \"SimpleRNN (frozen GloVe)\", \"accuracy\": acc_rnn, \"f1_macro\": f1_rnn},\n",
        "])\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b37d35",
      "metadata": {
        "id": "29b37d35"
      },
      "outputs": [],
      "source": [
        "\n",
        "cm = confusion_matrix(################)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['rec.autos','sci.electronics'])\n",
        "disp.plot(values_format='d')\n",
        "plt.title(\"Confusion matrix: RNN (frozen GloVe)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "372c8eb3",
      "metadata": {
        "id": "372c8eb3"
      },
      "source": [
        "\n",
        "## 8. Reflection (120–180 words)\n",
        "\n",
        "- Where did the averaged-embedding baseline do well, and where did the RNN improve?\n",
        "- Why do predictive embeddings help compared with raw counts or TF-IDF?  \n",
        "- If you fine-tuned the embedding layer (`trainable=True`) in this small-data setting, when would you expect gains or overfitting, and why?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FcRcJPR6HeaE",
      "metadata": {
        "id": "FcRcJPR6HeaE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
